{
  "model_weight_path": "best_tf_optimized.pt",
  "model_type": "OptimizedStitchedTransformer",
  "model_scale": "small",
  "architecture": "Speed-optimized transformer with mixed precision and gradient accumulation",
  "prob_th": 0.5,
  "min_len": 2,
  "win_sec": 4.0,
  "step_sec": 2.0,
  "fs": 400,
  "patch_size": 16,
  "d_model": 200,
  "num_layers": 6,
  "best_val_f1": 0.36795252176632715,
  "final_val_f1": 0.36795252176632715,
  "final_val_sens": 0.316326530610631,
  "final_val_ppv": 0.43971631205361905,
  "total_params": 2226202,
  "train_samples": 40453,
  "val_samples": 3656,
  "stopped_epoch": 5,
  "speed_optimizations": {
    "mixed_precision": true,
    "gradient_accumulation_steps": 4,
    "model_compiled": false,
    "effective_batch_size": 128
  },
  "training_params": {
    "data_folder": "D:\\datasets\\eeg\\dataset_processed\\wike25_tf",
    "augment": true,
    "train_split": 0.9,
    "data_percentage": 0.025,
    "model_scale": "small",
    "input_channels": 36,
    "patch_size": 16,
    "dropout": 0.05,
    "use_gradient_checkpointing": false,
    "epochs": 20,
    "batch_size": 32,
    "gradient_accumulation_steps": 4,
    "val_batch_size": 64,
    "learning_rate": 0.0005,
    "weight_decay": 0.01,
    "betas": [
      0.9,
      0.95
    ],
    "loss_type": "competition",
    "focal_alpha": 0.25,
    "focal_gamma": 2.0,
    "interval_weight": 2.0,
    "false_positive_penalty": 1.5,
    "warmup_ratio": 0.05,
    "min_lr_ratio": 0.01,
    "patience": 10,
    "gradient_clip_norm": 0.5,
    "num_workers": 2,
    "pin_memory": true,
    "non_blocking": true,
    "persistent_workers": true,
    "prefetch_factor": 4,
    "use_mixed_precision": true,
    "compile_model": false,
    "eval_frequency": 1,
    "save_frequency": 2,
    "log_frequency": 50,
    "seed": 2025,
    "save_model_path": "best_tf_optimized.pt",
    "save_metadata_path": "model_tf_optimized.json",
    "weights_folder": "weights/tf_optimized"
  }
}