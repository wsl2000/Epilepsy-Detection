import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import pyedflib
import pyedflib.highlevel as hl
from collections import defaultdict, Counter
from scipy import signal
from tqdm import tqdm
import multiprocessing as mp
from functools import partial
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

# è®¾ç½®å…¨å±€å­—ä½“ä¸º SimHei (é»‘ä½“) æˆ–å…¶ä»–ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']  # æˆ– ['Microsoft YaHei'] å¾®è½¯é›…é»‘ ç­‰
plt.rcParams['axes.unicode_minus'] = False   # è§£å†³è´Ÿå· '-' æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

class CHBMITDatasetAnalyzer:
    def __init__(self, 
                 original_path="/data/datasets/chb-mit-scalp-eeg-database-1.0.0",
                 processed_path="/data/datasets/BigDownstream/chb-mit/processed",
                 segmented_path="/data/datasets/BigDownstream/chb-mit/processed_seg",
                 output_dir="./analysis_results"):
        self.original_path = original_path
        self.processed_path = processed_path
        self.segmented_path = segmented_path
        self.output_dir = output_dir
        
        # Parameters from preprocessing scripts
        self.channels = [
            "FP1-F7", "F7-T7", "T7-P7", "P7-O1",
            "FP2-F8", "F8-T8", "T8-P8", "P8-O2",
            "FP1-F3", "F3-C3", "C3-P3", "P3-O1",
            "FP2-F4", "F4-C4", "C4-P4", "P4-O2"
        ]
        self.SAMPLING_RATE = 256
        self.test_pats = ["chb23", "chb24"]
        self.val_pats = ["chb21", "chb22"]
        self.train_pats = [
            "chb01", "chb02", "chb03", "chb04", "chb05", "chb06", "chb07", "chb08", "chb09", "chb10",
            "chb11", "chb12", "chb13", "chb14", "chb15", "chb16", "chb17", "chb18", "chb19", "chb20"
        ]
        
        # Processing parameters from process1.py
        self.process1_params = [
            ("01", "01", 2, 46, 0), ("02", "01", 2, 35, 0), ("03", "01", 2, 38, 0), ("05", "01", 2, 39, 0),
            ("06", "01", 2, 24, 0), ("07", "01", 2, 19, 0), ("08", "02", 3, 29, 0), ("10", "01", 2, 89, 0),
            ("11", "01", 2, 99, 0), ("14", "01", 2, 42, 0), ("20", "01", 2, 68, 0), ("21", "01", 2, 33, 0),
            ("22", "01", 2, 77, 0), ("23", "06", 7, 20, 0), ("24", "01", 3, 21, 0), ("04", "07", 1, 43, 1),
            ("09", "02", 1, 19, 1), ("15", "02", 1, 63, 1), ("16", "01", 2, 19, 0), ("18", "02", 1, 36, 1),
            ("19", "02", 1, 30, 1)
        ]
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def analyze_edf_file(self, edf_path):
        """Analyze a single EDF file"""
        try:
            signals, signal_headers, header = hl.read_edf(edf_path, digital=False)
            
            file_stats = {
                'file_size_mb': os.path.getsize(edf_path) / (1024 * 1024),
                'duration': header.get('Duration', 0) if isinstance(header, dict) else getattr(header, 'duration', 0),
                'channel_count': len(signal_headers),
                'sampling_rates': [],
                'channel_names': [],
                'signal_stats': []
            }
            
            # Analyze each channel
            for i, (sig, h) in enumerate(zip(signals, signal_headers)):
                if isinstance(h, dict):
                    sampling_rate = h.get('sample_rate') or h.get('sample_frequency') or self.SAMPLING_RATE
                    channel_name = h.get('label', f'Channel_{i}')
                else:
                    sampling_rate = getattr(h, 'sample_rate', None) or getattr(h, 'sample_frequency', None) or self.SAMPLING_RATE
                    channel_name = getattr(h, 'label', f'Channel_{i}')
                
                file_stats['sampling_rates'].append(sampling_rate)
                file_stats['channel_names'].append(channel_name)
                
                if len(sig) > 0:
                    file_stats['signal_stats'].append({
                        'channel': channel_name,
                        'min': np.min(sig),
                        'max': np.max(sig),
                        'mean': np.mean(sig),
                        'std': np.std(sig),
                        'median': np.median(sig),
                        'length': len(sig)
                    })
            
            return file_stats
        except Exception as e:
            return None

    def analyze_patient_original_data(self, patient):
        """åˆ†æå•ä¸ªæ‚£è€…çš„åŸå§‹æ•°æ®ï¼Œå†…éƒ¨æ–‡ä»¶å¹¶è¡Œå¤„ç†"""
        patient_path = os.path.join(self.original_path, patient)
        if not os.path.exists(patient_path):
            return None
        
        print(f"æ­£åœ¨åˆ†ææ‚£è€… {patient} çš„åŸå§‹æ•°æ®...")
        
        try:
            patient_files = [f for f in os.listdir(patient_path) if f.endswith('.edf')]
            edf_paths = [os.path.join(patient_path, f) for f in patient_files]
            
            # Parallel processing of EDF files within this patient
            with mp.Pool(mp.cpu_count()) as pool:
                file_results = list(tqdm(
                    pool.imap(self.analyze_edf_file, edf_paths),
                    total=len(edf_paths),
                    desc=f"  {patient} çš„EDFæ–‡ä»¶",
                    leave=False
                ))
            
            # Filter out None results
            valid_results = [result for result in file_results if result is not None]
            
            # Aggregate patient statistics
            patient_stats = {
                'patient': patient,
                'total_files': len(patient_files),
                'processed_files': len(valid_results),
                'total_duration': sum([r['duration'] for r in valid_results]),
                'total_size_mb': sum([r['file_size_mb'] for r in valid_results]),
                'channel_counts': [r['channel_count'] for r in valid_results],
                'sampling_rates': [],
                'channel_names': [],
                'signal_statistics': []
            }
            
            # Aggregate signal statistics
            for result in valid_results:
                patient_stats['sampling_rates'].extend(result['sampling_rates'])
                patient_stats['channel_names'].extend(result['channel_names'])
                patient_stats['signal_statistics'].extend(result['signal_stats'])
            
            # Parse seizure information
            summary_file = os.path.join(patient_path, f"{patient}-summary.txt")
            seizure_info = self._parse_summary_file_comprehensive(summary_file)
            patient_stats['seizure_info'] = seizure_info
            
            return patient_stats
            
        except Exception as e:
            print(f"Error analyzing {patient}: {e}")
            return None

    def analyze_patient_processed_data(self, patient):
        """åˆ†æå•ä¸ªæ‚£è€…çš„å¤„ç†åæ•°æ®"""
        patient_path = os.path.join(self.processed_path, patient)
        if not os.path.exists(patient_path):
            return None
        
        print(f"æ­£åœ¨åˆ†ææ‚£è€… {patient} çš„å¤„ç†åæ•°æ®...")
        
        try:
            pkl_files = [f for f in os.listdir(patient_path) if f.endswith('.pkl')]
            
            patient_stats = {
                'patient': patient,
                'total_files': len(pkl_files),
                'signal_statistics': [],
                'channel_names': set(),
                'sampling_rate': self.SAMPLING_RATE,
                'total_duration': 0
            }
            
            # Sample some files for analysis (not all to save time)
            sample_files = pkl_files[:min(5, len(pkl_files))]
            
            for pkl_file in tqdm(sample_files, desc=f"  {patient} çš„PKLæ–‡ä»¶", leave=False):
                try:
                    with open(os.path.join(patient_path, pkl_file), 'rb') as f:
                        data = pickle.load(f)
                    
                    # Analyze each channel's signal
                    for channel in self.channels:
                        if channel in data:
                            sig = data[channel]
                            patient_stats['channel_names'].add(channel)
                            patient_stats['signal_statistics'].append({
                                'channel': channel,
                                'min': np.min(sig),
                                'max': np.max(sig),
                                'mean': np.mean(sig),
                                'std': np.std(sig),
                                'length': len(sig)
                            })
                            
                    # Estimate duration from signal length
                    if len(patient_stats['signal_statistics']) > 0:
                        signal_length = patient_stats['signal_statistics'][0]['length']
                        patient_stats['total_duration'] += signal_length / self.SAMPLING_RATE
                
                except Exception as e:
                    continue
            
            patient_stats['channel_names'] = list(patient_stats['channel_names'])
            return patient_stats
            
        except Exception as e:
            print(f"Error analyzing processed data for {patient}: {e}")
            return None

    def analyze_patient_segmented_data(self, patient_split):
        """åˆ†æåˆ†å‰²åçš„æ•°æ®"""
        split_path = os.path.join(self.segmented_path, patient_split)
        if not os.path.exists(split_path):
            return None
        
        print(f"æ­£åœ¨åˆ†æ {patient_split} åˆ†å‰²çš„åˆ†æ®µæ•°æ®...")
        
        try:
            pkl_files = [f for f in os.listdir(split_path) if f.endswith('.pkl')]
            
            split_stats = {
                'split': patient_split,
                'total_files': len(pkl_files),
                'segment_shapes': [],
                'labels': [],
                'original_signal_stats': [],
                'processed_signal_stats': []
            }
            
            # Sample files for analysis
            sample_files = pkl_files[:min(100, len(pkl_files))]
            
            for pkl_file in tqdm(sample_files, desc=f"  {patient_split} çš„åˆ†æ®µ", leave=False):
                try:
                    with open(os.path.join(split_path, pkl_file), 'rb') as f:
                        data = pickle.load(f)
                    
                    X = data['X']  # Shape: (16, 2560)
                    y = data['y']
                    
                    split_stats['segment_shapes'].append(X.shape)
                    split_stats['labels'].append(y)
                    
                    # Original segment statistics
                    split_stats['original_signal_stats'].append({
                        'shape': X.shape,
                        'min': np.min(X),
                        'max': np.max(X),
                        'mean': np.mean(X),
                        'std': np.std(X),
                        'window_duration': X.shape[1] / self.SAMPLING_RATE,  # 10 seconds
                        'channels': X.shape[0]  # 16 channels
                    })
                    
                    # Simulate the final preprocessing from chb_dataset.py
                    X_resampled = signal.resample(X, 2000, axis=1)  # Resample to 2000
                    X_reshaped = X_resampled.reshape(19, 10, 200)   # Reshape to (19, 10, 200)
                    X_normalized = X_reshaped / 100                  # Normalize
                    
                    split_stats['processed_signal_stats'].append({
                        'original_shape': X.shape,
                        'resampled_shape': X_resampled.shape,
                        'final_shape': X_normalized.shape,
                        'final_min': np.min(X_normalized),
                        'final_max': np.max(X_normalized),
                        'final_mean': np.mean(X_normalized),
                        'final_std': np.std(X_normalized),
                        'final_sampling_rate': 2000 / 10,  # 200 Hz effective
                        'final_channels': 19,
                        'final_window_duration': 10  # seconds
                    })
                    
                except Exception as e:
                    continue
            
            return split_stats
            
        except Exception as e:
            print(f"Error analyzing segmented data for {patient_split}: {e}")
            return None

    def analyze_comprehensive_dataset(self):
        """ä½¿ç”¨é¡ºåºæ‚£è€…å¤„ç†å’Œå†…éƒ¨å¹¶è¡ŒåŒ–çš„ç»¼åˆåˆ†æ"""
        print("=== CHB-MITæ•°æ®é›†ç»¼åˆåˆ†æ ===")
        
        # 1. åˆ†æåŸå§‹æ•°æ®ï¼ˆæ‚£è€…é¡ºåºå¤„ç†ï¼Œæ¯ä¸ªæ‚£è€…å†…æ–‡ä»¶å¹¶è¡Œå¤„ç†ï¼‰
        print("\n1. åˆ†æåŸå§‹æ•°æ®...")
        available_patients = [f"chb{param[0]}" for param in self.process1_params]
        
        original_results = []
        for patient in tqdm(available_patients, desc="å¤„ç†æ‚£è€…ï¼ˆåŸå§‹æ•°æ®ï¼‰"):
            result = self.analyze_patient_original_data(patient)
            if result:
                original_results.append(result)
        
        # 2. åˆ†æå¤„ç†åæ•°æ®
        print("\n2. åˆ†æå¤„ç†åæ•°æ®...")
        processed_results = []
        if os.path.exists(self.processed_path):
            for patient in tqdm(available_patients, desc="å¤„ç†æ‚£è€…ï¼ˆå¤„ç†åæ•°æ®ï¼‰"):
                result = self.analyze_patient_processed_data(patient)
                if result:
                    processed_results.append(result)
        
        # 3. åˆ†æåˆ†æ®µæ•°æ®
        print("\n3. åˆ†æåˆ†æ®µæ•°æ®...")
        segmented_results = []
        if os.path.exists(self.segmented_path):
            for split in ['train', 'val', 'test']:
                result = self.analyze_patient_segmented_data(split)
                if result:
                    segmented_results.append(result)
        
        return self._aggregate_comprehensive_stats(original_results, processed_results, segmented_results)

    def _aggregate_comprehensive_stats(self, original_results, processed_results, segmented_results):
        """Aggregate statistics from all analysis results"""
        
        # Aggregate original data statistics
        original_stats = {
            'total_patients': len(original_results),
            'total_files': sum([r['total_files'] for r in original_results]),
            'total_duration_hours': sum([r['total_duration'] for r in original_results]) / 3600,
            'total_size_gb': sum([r['total_size_mb'] for r in original_results]) / 1024,
            'sampling_rate_distribution': Counter(),
            'channel_distribution': Counter(),
            'signal_amplitude_stats': {
                'all_minimums': [],
                'all_maximums': [],
                'all_means': [],
                'all_stds': []
            },
            'seizure_summary': {
                'total_seizures': 0,
                'patients_with_seizures': 0,
                'total_seizure_duration': 0
            }
        }
        
        for result in original_results:
            original_stats['sampling_rate_distribution'].update(result['sampling_rates'])
            original_stats['channel_distribution'].update(result['channel_names'])
            
            for sig_stat in result['signal_statistics']:
                original_stats['signal_amplitude_stats']['all_minimums'].append(sig_stat['min'])
                original_stats['signal_amplitude_stats']['all_maximums'].append(sig_stat['max'])
                original_stats['signal_amplitude_stats']['all_means'].append(sig_stat['mean'])
                original_stats['signal_amplitude_stats']['all_stds'].append(sig_stat['std'])
            
            seizure_info = result['seizure_info']
            original_stats['seizure_summary']['total_seizures'] += seizure_info['total_seizures']
            if seizure_info['total_seizures'] > 0:
                original_stats['seizure_summary']['patients_with_seizures'] += 1
            original_stats['seizure_summary']['total_seizure_duration'] += sum(seizure_info['seizure_durations'])
        
        # Aggregate processed data statistics
        processed_stats = {
            'total_patients': len(processed_results),
            'total_files': sum([r['total_files'] for r in processed_results]),
            'channels': len(self.channels),
            'sampling_rate': self.SAMPLING_RATE,
            'signal_amplitude_stats': {
                'all_minimums': [],
                'all_maximums': [],
                'all_means': [],
                'all_stds': []
            }
        }
        
        for result in processed_results:
            for sig_stat in result['signal_statistics']:
                processed_stats['signal_amplitude_stats']['all_minimums'].append(sig_stat['min'])
                processed_stats['signal_amplitude_stats']['all_maximums'].append(sig_stat['max'])
                processed_stats['signal_amplitude_stats']['all_means'].append(sig_stat['mean'])
                processed_stats['signal_amplitude_stats']['all_stds'].append(sig_stat['std'])
        
        # Aggregate segmented data statistics
        segmented_stats = {
            'splits': {},
            'total_segments': 0,
            'label_distribution': Counter(),
            'original_segment_stats': {
                'window_duration': 10,  # seconds
                'channels': 16,
                'sampling_rate': self.SAMPLING_RATE,
                'amplitude_stats': {'mins': [], 'maxs': [], 'means': [], 'stds': []}
            },
            'final_segment_stats': {
                'window_duration': 10,  # seconds
                'channels': 19,
                'sampling_rate': 200,  # effective Hz
                'shape': '(19, 10, 200)',
                'amplitude_stats': {'mins': [], 'maxs': [], 'means': [], 'stds': []}
            }
        }
        
        for result in segmented_results:
            split_name = result['split']
            segmented_stats['splits'][split_name] = {
                'total_files': result['total_files'],
                'labels': Counter(result['labels'])
            }
            segmented_stats['total_segments'] += result['total_files']
            segmented_stats['label_distribution'].update(result['labels'])
            
            # Aggregate signal statistics
            for orig_stat in result['original_signal_stats']:
                segmented_stats['original_segment_stats']['amplitude_stats']['mins'].append(orig_stat['min'])
                segmented_stats['original_segment_stats']['amplitude_stats']['maxs'].append(orig_stat['max'])
                segmented_stats['original_segment_stats']['amplitude_stats']['means'].append(orig_stat['mean'])
                segmented_stats['original_segment_stats']['amplitude_stats']['stds'].append(orig_stat['std'])
            
            for proc_stat in result['processed_signal_stats']:
                segmented_stats['final_segment_stats']['amplitude_stats']['mins'].append(proc_stat['final_min'])
                segmented_stats['final_segment_stats']['amplitude_stats']['maxs'].append(proc_stat['final_max'])
                segmented_stats['final_segment_stats']['amplitude_stats']['means'].append(proc_stat['final_mean'])
                segmented_stats['final_segment_stats']['amplitude_stats']['stds'].append(proc_stat['final_std'])
        
        return original_stats, processed_stats, segmented_stats

    def _parse_summary_file_comprehensive(self, summary_path):
        """Comprehensive parsing of summary files"""
        seizure_info = {
            'file_seizure_counts': [],
            'seizure_durations': [],
            'total_seizures': 0,
            'files_with_seizures': 0
        }
        
        if not os.path.exists(summary_path):
            return seizure_info
            
        try:
            with open(summary_path, 'r') as f:
                lines = f.readlines()
                
            current_file_seizures = 0
            for i in range(len(lines)):
                line = lines[i].split()
                if len(line) >= 3 and line[2].endswith('.edf'):
                    # Reset for new file
                    current_file_seizures = 0
                    
                    # Find seizure count for this file
                    j = i + 1
                    while j < len(lines):
                        if lines[j].split() and len(lines[j].split()) > 0 and lines[j].split()[0] == "Number":
                            try:
                                current_file_seizures = int(lines[j].split()[-1])
                                seizure_info['file_seizure_counts'].append(current_file_seizures)
                                seizure_info['total_seizures'] += current_file_seizures
                                if current_file_seizures > 0:
                                    seizure_info['files_with_seizures'] += 1
                                break
                            except:
                                pass
                        j += 1
                        
                    # Get seizure durations if seizures > 0
                    if current_file_seizures > 0:
                        j = i + 1
                        seizure_count = 0
                        while j < len(lines) and seizure_count < current_file_seizures:
                            l = lines[j].split()
                            if len(l) > 0 and l[0] == "Seizure" and "Start" in l:
                                try:
                                    start = int(l[-2])
                                    if j + 1 < len(lines):
                                        end = int(lines[j + 1].split()[-2])
                                        duration = end - start
                                        seizure_info['seizure_durations'].append(duration)
                                        seizure_count += 1
                                except:
                                    pass
                            j += 1
        except Exception as e:
            print(f"Error parsing summary file {summary_path}: {e}")
                    
        return seizure_info
    
    def analyze_preprocessing_pipeline(self):
        """Analyze preprocessing pipeline from code"""
        print("\n=== ANALYZING PREPROCESSING PIPELINE ===")
        
        # Calculate expected output from process2.py
        total_files_processed = len([p for p in self.process1_params])
        
        # Estimate segments per patient based on process1.py parameters
        estimated_segments = {
            'train': 0,
            'val': 0,
            'test': 0
        }
        
        estimated_seizure_segments = 0
        estimated_normal_segments = 0
        
        for patient_param in self.process1_params:
            patient = f"chb{patient_param[0]}"
            start_file = patient_param[2]
            end_file = patient_param[3]
            num_files = end_file - start_file + 1
            
            # Estimate 1 hour average per file, 360 segments per hour (10-second segments)
            estimated_segments_per_patient = num_files * 360
            
            if patient in self.test_pats:
                estimated_segments['test'] += estimated_segments_per_patient
            elif patient in self.val_pats:
                estimated_segments['val'] += estimated_segments_per_patient
            else:
                estimated_segments['train'] += estimated_segments_per_patient
        
        # Processing steps analysis
        pipeline_stats = {
            'stage1_process1': {
                'input_format': 'EDF files',
                'output_format': 'PKL files',
                'channel_selection': len(self.channels),
                'patients_processed': len(self.process1_params),
                'compression': 'pickle',
                'metadata_extraction': True
            },
            'stage2_process2': {
                'segmentation': '10-second windows',
                'segment_length_samples': 10 * self.SAMPLING_RATE,
                'overlap': 'Non-overlapping + seizure augmentation',
                'augmentation': '5-second sliding window for seizures',
                'labeling': 'Binary (seizure/non-seizure)',
                'estimated_total_segments': sum(estimated_segments.values())
            },
            'stage3_chb_dataset': {
                'resampling': f'{10 * self.SAMPLING_RATE} â†’ 2000 samples',
                'reshape': 'From (16, 2000) to (19, 10, 200)',
                'channel_padding': '16 â†’ 19 channels (zero padding)',
                'normalization': 'Division by 100',
                'final_shape': '(19, 10, 200)',
                'effective_sampling_rate': '200 Hz'
            }
        }
        
        return estimated_segments, pipeline_stats
    
    def _summarize_comprehensive_stats(self, stats):
        """Comprehensive summary of original dataset"""
        # Channel analysis
        channel_stats = {}
        for channel_name, channel_data in stats['signal_statistics_per_channel'].items():
            if channel_data:
                channel_stats[channel_name] = {
                    'count': len(channel_data),
                    'avg_amplitude_range': np.mean([d['max'] - d['min'] for d in channel_data]),
                    'avg_mean': np.mean([d['mean'] for d in channel_data]),
                    'avg_std': np.mean([d['std'] for d in channel_data])
                }
        
        summary = {
            'dataset_overview': {
                'total_patients': len(stats['patients']),
                'total_files': stats['total_files'],
                'avg_files_per_patient': np.mean(stats['files_per_patient']) if stats['files_per_patient'] else 0,
                'total_recording_hours': sum(stats['recording_hours']),
                'avg_recording_hours_per_patient': np.mean(stats['recording_hours']) if stats['recording_hours'] else 0
            },
            'technical_specs': {
                'sampling_rate': self.SAMPLING_RATE,
                'common_sampling_rate': Counter(stats['sampling_rates']).most_common(1)[0] if stats['sampling_rates'] else (None, 0),
                'avg_duration_per_file': np.mean(stats['durations']) if stats['durations'] else 0,
                'avg_channels_per_file': np.mean(stats['channel_counts']) if stats['channel_counts'] else 0,
                'avg_file_size_mb': np.mean(stats['file_sizes']) if stats['file_sizes'] else 0
            },
            'channel_analysis': {
                'unique_channels': len(set(stats['channel_names'])),
                'most_common_channels': Counter(stats['channel_names']).most_common(20),
                'target_channels': self.channels,
                'target_channel_count': len(self.channels)
            },
            'signal_characteristics': {
                'overall_amplitude_range': {
                    'min': np.min(stats['signal_amplitudes']) if stats['signal_amplitudes'] else 0,
                    'max': np.max(stats['signal_amplitudes']) if stats['signal_amplitudes'] else 0,
                    'mean': np.mean(stats['signal_amplitudes']) if stats['signal_amplitudes'] else 0,
                    'std': np.std(stats['signal_amplitudes']) if stats['signal_amplitudes'] else 0
                },
                'per_channel_stats': channel_stats
            },
            'seizure_analysis': {
                'total_seizures': sum(stats['seizure_counts']),
                'files_with_seizures': len([x for x in stats['seizure_counts'] if x > 0]),
                'files_without_seizures': len([x for x in stats['seizure_counts'] if x == 0]),
                'avg_seizures_per_file': np.mean(stats['seizure_counts']) if stats['seizure_counts'] else 0,
                'seizure_duration_stats': {
                    'avg_duration': np.mean(stats['seizure_durations']) if stats['seizure_durations'] else 0,
                    'min_duration': np.min(stats['seizure_durations']) if stats['seizure_durations'] else 0,
                    'max_duration': np.max(stats['seizure_durations']) if stats['seizure_durations'] else 0,
                    'total_seizure_time': sum(stats['seizure_durations']) if stats['seizure_durations'] else 0
                }
            }
        }
        return summary
    
    def generate_comprehensive_documentation(self):
        """ç”Ÿæˆå®Œæ•´æ–‡æ¡£"""
        print("æ­£åœ¨ç”ŸæˆCHB-MITæ•°æ®é›†ç»¼åˆæ–‡æ¡£...")
        
        # åˆ†ææ‰€æœ‰æ•°æ®é˜¶æ®µ
        original_stats, processed_stats, segmented_stats = self.analyze_comprehensive_dataset()
        
        # ä»ä»£ç åˆ†æé¢„å¤„ç†æµæ°´çº¿
        estimated_segments, pipeline_stats = self.analyze_preprocessing_pipeline()
        
        # ç”Ÿæˆæ–‡æ¡£
        doc_content = self._create_detailed_markdown_doc(original_stats, processed_stats, segmented_stats, pipeline_stats)
        
        # ä¿å­˜æ–‡æ¡£
        with open(os.path.join(self.output_dir, 'CHB_MIT_è¯¦ç»†åˆ†æ.md'), 'w', encoding='utf-8') as f:
            f.write(doc_content)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._create_detailed_visualizations(original_stats, processed_stats, segmented_stats)
        
        print(f"è¯¦ç»†æ–‡æ¡£å·²ä¿å­˜åˆ° {self.output_dir}")
        return original_stats, processed_stats, segmented_stats, pipeline_stats

    def _create_detailed_markdown_doc(self, original_stats, processed_stats, segmented_stats, pipeline_stats):
        """åˆ›å»ºè¯¦ç»†çš„ä¸­æ–‡markdownæ–‡æ¡£"""
        
        # Calculate summary statistics
        def calc_stats(values):
            if not values:
                return {'min': 0, 'max': 0, 'mean': 0, 'std': 0, 'median': 0}
            return {
                'min': np.min(values),
                'max': np.max(values),
                'mean': np.mean(values),
                'std': np.std(values),
                'median': np.median(values)
            }
        
        orig_amp_stats = calc_stats(original_stats['signal_amplitude_stats']['all_means'])
        proc_amp_stats = calc_stats(processed_stats['signal_amplitude_stats']['all_means'])
        
        doc = f"""# CHB-MITæ•°æ®é›†è¯¦ç»†åˆ†ææŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦

æœ¬ç»¼åˆåˆ†ææ£€æŸ¥äº†CHB-MITæ•°æ®é›†åœ¨æ‰€æœ‰å¤„ç†é˜¶æ®µçš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚

### æ•°æ®é›†æ¦‚è§ˆ
- **åŸå§‹æ‚£è€…æ•°**: {original_stats['total_patients']}
- **åŸå§‹æ–‡ä»¶æ•°**: {original_stats['total_files']}
- **æ€»å½•åˆ¶æ—¶é—´**: {original_stats['total_duration_hours']:.1f} å°æ—¶
- **æ€»æ•°æ®å¤§å°**: {original_stats['total_size_gb']:.1f} GB
- **æ€»ç™«ç—«å‘ä½œæ¬¡æ•°**: {original_stats['seizure_summary']['total_seizures']}

## 1. åŸå§‹æ•°æ®åˆ†æ

### åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
- **åˆ†ææ‚£è€…æ•°**: {original_stats['total_patients']}
- **EDFæ–‡ä»¶æ€»æ•°**: {original_stats['total_files']}
- **æ€»å½•åˆ¶æ—¶é•¿**: {original_stats['total_duration_hours']:.1f} å°æ—¶
- **æ•°æ®é›†æ€»å¤§å°**: {original_stats['total_size_gb']:.1f} GB
- **æ¯æ‚£è€…å¹³å‡**: {original_stats['total_duration_hours']/max(original_stats['total_patients'], 1):.1f} å°æ—¶

### é‡‡æ ·ç‡åˆ†å¸ƒ
"""
        
        for rate, count in original_stats['sampling_rate_distribution'].most_common(5):
            doc += f"- {rate} Hz: {count} ä¸ªé€šé“ ({count/sum(original_stats['sampling_rate_distribution'].values())*100:.1f}%)\n"
        
        doc += f"""

### ä¿¡å·å¹…åº¦ç»Ÿè®¡ï¼ˆåŸå§‹ï¼‰
- **èŒƒå›´**: {orig_amp_stats['min']:.2f} åˆ° {orig_amp_stats['max']:.2f} ÂµV
- **å‡å€¼**: {orig_amp_stats['mean']:.2f} ÂµV
- **æ ‡å‡†å·®**: {orig_amp_stats['std']:.2f} ÂµV
- **ä¸­ä½æ•°**: {orig_amp_stats['median']:.2f} ÂµV

### é€šé“åˆ†å¸ƒï¼ˆå‰15åï¼‰
"""
        
        for i, (channel, count) in enumerate(original_stats['channel_distribution'].most_common(15)):
            doc += f"{i+1:2d}. {channel}: {count} æ¬¡å‡ºç°\n"
        
        doc += f"""

### ç™«ç—«å‘ä½œåˆ†æ
- **æ€»å‘ä½œæ¬¡æ•°**: {original_stats['seizure_summary']['total_seizures']}
- **æœ‰å‘ä½œçš„æ‚£è€…**: {original_stats['seizure_summary']['patients_with_seizures']}/{original_stats['total_patients']}
- **æ€»å‘ä½œæ—¶é•¿**: {original_stats['seizure_summary']['total_seizure_duration']:.1f} ç§’
- **å‘ä½œç‡**: {original_stats['seizure_summary']['total_seizure_duration']/3600/max(original_stats['total_duration_hours'], 1)*100:.2f}% çš„å½•åˆ¶æ—¶é—´

## 2. å¤„ç†åæ•°æ®åˆ†æ

### å¤„ç†ç»Ÿè®¡ä¿¡æ¯
- **å¤„ç†çš„æ‚£è€…æ•°**: {processed_stats['total_patients']}
- **PKLæ–‡ä»¶æ€»æ•°**: {processed_stats['total_files']}
- **ç›®æ ‡é€šé“æ•°**: {processed_stats['channels']} (æ ‡å‡†åŒ–)
- **é‡‡æ ·ç‡**: {processed_stats['sampling_rate']} Hz (ä¿æŒä¸å˜)

### ä¿¡å·å¹…åº¦ç»Ÿè®¡ï¼ˆé€šé“é€‰æ‹©åï¼‰
- **èŒƒå›´**: {proc_amp_stats['min']:.2f} åˆ° {proc_amp_stats['max']:.2f} ÂµV
- **å‡å€¼**: {proc_amp_stats['mean']:.2f} ÂµV
- **æ ‡å‡†å·®**: {proc_amp_stats['std']:.2f} ÂµV
- **ä¸­ä½æ•°**: {proc_amp_stats['median']:.2f} ÂµV

### ç›®æ ‡é€šé“ï¼ˆæ¥è‡ªprocess2.pyï¼‰
"""
        
        for i, channel in enumerate(self.channels, 1):
            doc += f"{i:2d}. {channel}\n"
        
        doc += f"""

## 3. åˆ†æ®µæ•°æ®åˆ†æ

### æ•°æ®é›†åˆ’åˆ†
"""
        
        for split_name, split_data in segmented_stats['splits'].items():
            split_names = {'train': 'è®­ç»ƒ', 'val': 'éªŒè¯', 'test': 'æµ‹è¯•'}
            doc += f"- **{split_names.get(split_name, split_name)}**: {split_data['total_files']:,} ä¸ªåˆ†æ®µ\n"
        
        doc += f"- **æ€»è®¡**: {segmented_stats['total_segments']:,} ä¸ªåˆ†æ®µ\n\n"
        
        doc += f"""### æ ‡ç­¾åˆ†å¸ƒ
- **éç™«ç—«(0)**: {segmented_stats['label_distribution'].get(0, 0):,} ä¸ªåˆ†æ®µ ({segmented_stats['label_distribution'].get(0, 0)/max(segmented_stats['total_segments'], 1)*100:.1f}%)
- **ç™«ç—«(1)**: {segmented_stats['label_distribution'].get(1, 0):,} ä¸ªåˆ†æ®µ ({segmented_stats['label_distribution'].get(1, 0)/max(segmented_stats['total_segments'], 1)*100:.1f}%)

### åŸå§‹åˆ†æ®µç‰¹å¾ï¼ˆæœ€ç»ˆå¤„ç†å‰ï¼‰
- **çª—å£æ—¶é•¿**: {segmented_stats['original_segment_stats']['window_duration']} ç§’
- **é€šé“æ•°**: {segmented_stats['original_segment_stats']['channels']}
- **é‡‡æ ·ç‡**: {segmented_stats['original_segment_stats']['sampling_rate']} Hz
- **æ¯åˆ†æ®µæ ·æœ¬æ•°**: {segmented_stats['original_segment_stats']['window_duration'] * segmented_stats['original_segment_stats']['sampling_rate']}

#### åŸå§‹åˆ†æ®µå¹…åº¦ç»Ÿè®¡
"""
        
        if segmented_stats['original_segment_stats']['amplitude_stats']['means']:
            orig_seg_stats = calc_stats(segmented_stats['original_segment_stats']['amplitude_stats']['means'])
            doc += f"""- **èŒƒå›´**: {orig_seg_stats['min']:.2f} åˆ° {orig_seg_stats['max']:.2f} ÂµV
- **å‡å€¼**: {orig_seg_stats['mean']:.2f} ÂµV
- **æ ‡å‡†å·®**: {orig_seg_stats['std']:.2f} ÂµV
"""
        
        doc += f"""

### æœ€ç»ˆå¤„ç†ç‰¹å¾ï¼ˆchb_dataset.pyåï¼‰
- **æœ€ç»ˆå½¢çŠ¶**: {segmented_stats['final_segment_stats']['shape']}
- **æœ‰æ•ˆé‡‡æ ·ç‡**: {segmented_stats['final_segment_stats']['sampling_rate']} Hz
- **é€šé“æ•°**: {segmented_stats['final_segment_stats']['channels']} (ä»{segmented_stats['original_segment_stats']['channels']}å¡«å……)
- **çª—å£æ—¶é•¿**: {segmented_stats['final_segment_stats']['window_duration']} ç§’

#### æœ€ç»ˆå¹…åº¦ç»Ÿè®¡ï¼ˆå½’ä¸€åŒ–åï¼‰
"""
        
        if segmented_stats['final_segment_stats']['amplitude_stats']['means']:
            final_seg_stats = calc_stats(segmented_stats['final_segment_stats']['amplitude_stats']['means'])
            doc += f"""- **èŒƒå›´**: {final_seg_stats['min']:.3f} åˆ° {final_seg_stats['max']:.3f} (å½’ä¸€åŒ–)
- **å‡å€¼**: {final_seg_stats['mean']:.3f} (å½’ä¸€åŒ–)
- **æ ‡å‡†å·®**: {final_seg_stats['std']:.3f} (å½’ä¸€åŒ–)
"""
        
        doc += f"""

## 4. å¤„ç†æµæ°´çº¿æ€»ç»“

### ç¬¬ä¸€é˜¶æ®µ: EDF â†’ PKL (process1.py)
- **è¾“å…¥**: {original_stats['total_files']} ä¸ªEDFæ–‡ä»¶
- **è¾“å‡º**: {processed_stats['total_files']} ä¸ªPKLæ–‡ä»¶
- **é€šé“é€‰æ‹©**: {len(original_stats['channel_distribution'])} â†’ {processed_stats['channels']} ä¸ªé€šé“
- **å‹ç¼©**: Pickleæ ¼å¼å¸¦å…ƒæ•°æ®

### ç¬¬äºŒé˜¶æ®µ: PKL â†’ åˆ†æ®µ (process2.py)
- **è¾“å…¥**: è¿ç»­ä¿¡å·çš„PKLæ–‡ä»¶
- **åˆ†æ®µ**: {segmented_stats['original_segment_stats']['window_duration']}ç§’éé‡å çª—å£
- **å¢å¼º**: ç™«ç—«æœŸé—´5ç§’æ»‘åŠ¨çª—å£
- **è¾“å‡º**: {segmented_stats['total_segments']:,} ä¸ªåˆ†æ®µ
- **æ ‡æ³¨**: äºŒè¿›åˆ¶ (0=æ­£å¸¸, 1=ç™«ç—«)

### ç¬¬ä¸‰é˜¶æ®µ: æœ€ç»ˆå¤„ç† (chb_dataset.py)
- **é‡é‡‡æ ·**: {segmented_stats['original_segment_stats']['window_duration'] * segmented_stats['original_segment_stats']['sampling_rate']} â†’ 2000 æ ·æœ¬
- **é€šé“å¡«å……**: {segmented_stats['original_segment_stats']['channels']} â†’ {segmented_stats['final_segment_stats']['channels']} é€šé“
- **é‡å¡‘**: (16, 2000) â†’ {segmented_stats['final_segment_stats']['shape']}
- **å½’ä¸€åŒ–**: é™¤ä»¥100

## 5. æ•°æ®å˜æ¢æ€»ç»“

```
åŸå§‹EDF:
â”œâ”€ é‡‡æ ·ç‡: {list(original_stats['sampling_rate_distribution'].keys())[0] if original_stats['sampling_rate_distribution'] else 'N/A'} Hz
â”œâ”€ é€šé“: {len(original_stats['channel_distribution'])} ç§ç‹¬ç‰¹ç±»å‹
â”œâ”€ å¹…åº¦: {orig_amp_stats['mean']:.1f} Â± {orig_amp_stats['std']:.1f} ÂµV
â””â”€ æ—¶é•¿: {original_stats['total_duration_hours']:.1f} å°æ—¶

å¤„ç†åPKL:
â”œâ”€ é‡‡æ ·ç‡: {processed_stats['sampling_rate']} Hz (ä¿æŒ)
â”œâ”€ é€šé“: {processed_stats['channels']} (æ ‡å‡†åŒ–)
â”œâ”€ å¹…åº¦: {proc_amp_stats['mean']:.1f} Â± {proc_amp_stats['std']:.1f} ÂµV
â””â”€ æ ¼å¼: å¸¦å…ƒæ•°æ®çš„Pickle

åˆ†æ®µ:
â”œâ”€ çª—å£å¤§å°: {segmented_stats['original_segment_stats']['window_duration']}ç§’ Ã— {segmented_stats['original_segment_stats']['channels']} é€šé“
â”œâ”€ æ ·æœ¬: æ¯åˆ†æ®µ{segmented_stats['original_segment_stats']['window_duration'] * segmented_stats['original_segment_stats']['sampling_rate']}ä¸ª
â”œâ”€ æ€»åˆ†æ®µæ•°: {segmented_stats['total_segments']:,}
â””â”€ æ ‡ç­¾: {segmented_stats['label_distribution'].get(1, 0):,} ç™«ç—«, {segmented_stats['label_distribution'].get(0, 0):,} æ­£å¸¸

æœ€ç»ˆå¼ é‡:
â”œâ”€ å½¢çŠ¶: {segmented_stats['final_segment_stats']['shape']}
â”œâ”€ é‡‡æ ·ç‡: {segmented_stats['final_segment_stats']['sampling_rate']} Hz (æœ‰æ•ˆ)
â”œâ”€ å¹…åº¦: å½’ä¸€åŒ– (Ã·100)
â””â”€ å†…å­˜: ~{segmented_stats['total_segments'] * 19 * 10 * 200 * 4 / 1024 / 1024 / 1024:.1f} GB
```

## 6. å…³é”®æ´å¯Ÿ

### æ•°æ®è´¨é‡
1. **é‡‡æ ·ä¸€è‡´æ€§**: {list(original_stats['sampling_rate_distribution'].most_common(1))[0][1]/sum(original_stats['sampling_rate_distribution'].values())*100:.1f}% çš„é€šé“ä½¿ç”¨ä¸»è¦é‡‡æ ·ç‡
2. **é€šé“æ ‡å‡†åŒ–**: é€šé“å˜å¼‚æ€§ä»{len(original_stats['channel_distribution'])}ç§å‡å°‘åˆ°{processed_stats['channels']}ç§ç±»å‹
3. **ç™«ç—«è¡¨ç¤º**: {segmented_stats['label_distribution'].get(1, 0)/max(segmented_stats['total_segments'], 1)*100:.1f}% ç™«ç—«åˆ†æ®µ (ç±»åˆ«ä¸å¹³è¡¡)

### å¤„ç†å½±å“
1. **æ•°æ®ç¼©å‡**: {segmented_stats['final_segment_stats']['sampling_rate']}/{segmented_stats['original_segment_stats']['sampling_rate']} = {segmented_stats['final_segment_stats']['sampling_rate']/segmented_stats['original_segment_stats']['sampling_rate']:.2f}Ã— é‡‡æ ·ç‡ç¼©å‡
2. **é€šé“å¡«å……**: é€šè¿‡é›¶å¡«å……å¢åŠ {segmented_stats['final_segment_stats']['channels'] - segmented_stats['original_segment_stats']['channels']}ä¸ªé€šé“
3. **å¹…åº¦å½’ä¸€åŒ–**: ä¿¡å·ç¼©æ”¾åˆ°ç¥ç»ç½‘ç»œå‹å¥½èŒƒå›´

### å»ºè®®
1. **ç±»åˆ«ä¸å¹³è¡¡**: ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°æˆ–æ•°æ®å¢å¼º
2. **å†…å­˜ä¼˜åŒ–**: å¯¹å¤§æ•°æ®é›†è€ƒè™‘æ‰¹é‡åŠ è½½
3. **éªŒè¯**: æŒ‰æ‚£è€…åˆ’åˆ†é˜²æ­¢æ•°æ®æ³„éœ²
4. **ç›‘æ§**: è·Ÿè¸ªæ¯é€šé“ç»Ÿè®¡ä»¥è¿›è¡Œè´¨é‡æ§åˆ¶
"""
        
        return doc

    def _create_detailed_visualizations(self, original_stats, processed_stats, segmented_stats):
        """åˆ›å»ºè¯¦ç»†çš„å¯è§†åŒ–å›¾è¡¨"""
        plt.style.use('default')
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # Create comprehensive visualization
        fig = plt.figure(figsize=(24, 18))
        
        # Plot 1: é‡‡æ ·ç‡åˆ†å¸ƒ
        ax1 = plt.subplot(4, 4, 1)
        if original_stats['sampling_rate_distribution']:
            rates, counts = zip(*original_stats['sampling_rate_distribution'].most_common(5))
            ax1.bar(range(len(rates)), counts, alpha=0.7)
            ax1.set_title('é‡‡æ ·ç‡åˆ†å¸ƒï¼ˆåŸå§‹ï¼‰')
            ax1.set_xlabel('é‡‡æ ·ç‡ (Hz)')
            ax1.set_ylabel('é€šé“æ•°')
            ax1.set_xticks(range(len(rates)))
            ax1.set_xticklabels([f'{r}' for r in rates])
        
        # Plot 2: é€šé“åˆ†å¸ƒ
        ax2 = plt.subplot(4, 4, 2)
        if original_stats['channel_distribution']:
            channels, counts = zip(*original_stats['channel_distribution'].most_common(10))
            ax2.barh(range(len(channels)), counts, alpha=0.7)
            ax2.set_title('å‰10ä¸ªé€šé“ï¼ˆåŸå§‹ï¼‰')
            ax2.set_xlabel('é¢‘ç‡')
            ax2.set_yticks(range(len(channels)))
            ax2.set_yticklabels(channels, fontsize=8)
        
        # Plot 3: å¹…åº¦åˆ†å¸ƒæ¯”è¾ƒ
        ax3 = plt.subplot(4, 4, 3)
        stages = ['åŸå§‹', 'å¤„ç†å', 'æœ€ç»ˆ']
        if (original_stats['signal_amplitude_stats']['all_means'] and 
            processed_stats['signal_amplitude_stats']['all_means'] and 
            segmented_stats['final_segment_stats']['amplitude_stats']['means']):
            
            means = [
                np.mean(original_stats['signal_amplitude_stats']['all_means']),
                np.mean(processed_stats['signal_amplitude_stats']['all_means']),
                np.mean(segmented_stats['final_segment_stats']['amplitude_stats']['means'])
            ]
            ax3.plot(stages, means, 'o-', linewidth=2, markersize=8)
            ax3.set_title('å„é˜¶æ®µä¿¡å·å¹…åº¦')
            ax3.set_ylabel('å¹³å‡å¹…åº¦')
            ax3.set_yscale('log')
        
        # Plot 4: æ•°æ®é›†åˆ’åˆ†
        ax4 = plt.subplot(4, 4, 4)
        if segmented_stats['splits']:
            splits = list(segmented_stats['splits'].keys())
            split_names = {'train': 'è®­ç»ƒ', 'val': 'éªŒè¯', 'test': 'æµ‹è¯•'}
            labels = [split_names.get(s, s) for s in splits]
            counts = [segmented_stats['splits'][s]['total_files'] for s in splits]
            ax4.pie(counts, labels=labels, autopct='%1.1f%%')
            ax4.set_title('æ•°æ®é›†åˆ’åˆ†åˆ†å¸ƒ')
        
        # Plot 5: æ ‡ç­¾åˆ†å¸ƒ
        ax5 = plt.subplot(4, 4, 5)
        if segmented_stats['label_distribution']:
            labels = ['æ­£å¸¸', 'ç™«ç—«']
            counts = [segmented_stats['label_distribution'].get(0, 0), 
                     segmented_stats['label_distribution'].get(1, 0)]
            colors = ['lightblue', 'red']
            ax5.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%')
            ax5.set_title('æ ‡ç­¾åˆ†å¸ƒ')
        
        # Plot 6: å¤„ç†æµæ°´çº¿æµç¨‹
        ax6 = plt.subplot(4, 4, 6)
        stages = ['åŸå§‹\nEDF', 'å¤„ç†å\nPKL', 'åˆ†æ®µ\næ–‡ä»¶', 'æœ€ç»ˆ\nå¼ é‡']
        values = [
            original_stats['total_files'],
            processed_stats['total_files'],
            segmented_stats['total_segments'],
            segmented_stats['total_segments']
        ]
        ax6.plot(stages, values, 'o-', linewidth=2, markersize=8)
        ax6.set_title('æ•°æ®å¤„ç†æµç¨‹')
        ax6.set_ylabel('æ•°é‡')
        ax6.tick_params(axis='x', rotation=45)
        
        # Plot 7: ç™«ç—«åˆ†æ
        ax7 = plt.subplot(4, 4, 7)
        seizure_data = [
            original_stats['seizure_summary']['patients_with_seizures'],
            original_stats['total_patients'] - original_stats['seizure_summary']['patients_with_seizures']
        ]
        ax7.pie(seizure_data, labels=['æœ‰ç™«ç—«', 'æ— ç™«ç—«'], autopct='%1.1f%%')
        ax7.set_title('æŒ‰ç™«ç—«å­˜åœ¨åˆ’åˆ†çš„æ‚£è€…')
        
        # Plot 8: ä¿¡å·å½¢çŠ¶å˜æ¢
        ax8 = plt.subplot(4, 4, 8)
        shapes = ['åŸå§‹\n(å¯å˜)', 'åˆ†æ®µ\n(16Ã—2560)', 'æœ€ç»ˆ\n(19Ã—10Ã—200)']
        complexities = [1, 16*2560, 19*10*200]  # ç›¸å¯¹å¤æ‚åº¦
        ax8.bar(shapes, complexities, alpha=0.7, color=['blue', 'orange', 'green'])
        ax8.set_title('ä¿¡å·å½¢çŠ¶å˜æ¢')
        ax8.set_ylabel('æ¯æ ·æœ¬æ•°æ®ç‚¹')
        ax8.set_yscale('log')
        
        # Plot 9-12: Detailed statistics plots
        # ... (additional plots for detailed statistics)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'chb_mit_è¯¦ç»†åˆ†æ.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print("è¯¦ç»†å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° chb_mit_è¯¦ç»†åˆ†æ.png")

# Usage example
if __name__ == "__main__":
    analyzer = CHBMITDatasetAnalyzer(r"D:\datasets\eeg\dataset_dir_original\chb-mit-scalp-eeg-database-1.0.0")
    original_stats, processed_stats, segmented_stats, pipeline_stats = analyzer.generate_comprehensive_documentation()
    
    print("\n" + "="*60)
    print("CHB-MITæ•°æ®é›†è¯¦ç»†åˆ†æå®Œæˆ")
    print("="*60)
    print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {analyzer.output_dir}")
    print(f"ğŸ“Š åŸå§‹æ‚£è€…æ•°: {original_stats['total_patients']}")
    print(f"ğŸ“‚ åŸå§‹æ–‡ä»¶æ•°: {original_stats['total_files']}")
    print(f"â±ï¸ æ€»å½•åˆ¶æ—¶é—´: {original_stats['total_duration_hours']:.1f} å°æ—¶")
    print(f"ğŸ’¾ æ€»æ•°æ®å¤§å°: {original_stats['total_size_gb']:.1f} GB")
    print(f"ğŸ§  æ€»ç™«ç—«å‘ä½œ: {original_stats['seizure_summary']['total_seizures']}")
    
    if segmented_stats['total_segments'] > 0:
        seizure_ratio = segmented_stats['label_distribution'].get(1, 0) / segmented_stats['total_segments'] * 100
        print(f"ğŸ“ˆ æ€»åˆ†æ®µæ•°: {segmented_stats['total_segments']:,}")
        print(f"âš–ï¸ ç™«ç—«æ¯”ä¾‹: {seizure_ratio:.2f}%")
    else:
        print(f"ğŸ“ˆ ä¼°è®¡åˆ†æ®µæ•°: {sum(estimated_segments.values()):,}")
        print(f"âš–ï¸ ç™«ç—«æ¯”ä¾‹: åŸºäºä»£ç åˆ†æä¼°è®¡")
    
    print(f"ğŸ”§ å¤„ç†é˜¶æ®µå·²åˆ†æ: 3ä¸ªå®Œæ•´é˜¶æ®µ")
    print("="*60)
    
    # ç”Ÿæˆåˆ†æå®Œæˆçš„æ€»ç»“
    print("\nğŸ“ åˆ†ææ€»ç»“:")
    if original_stats['total_files'] > 0:
        print("âœ… åŸå§‹EDFæ–‡ä»¶: æˆåŠŸåˆ†æï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†")
        print(f"   - å‘ç° {original_stats['total_files']} ä¸ªæ–‡ä»¶æ¥è‡ª {original_stats['total_patients']} ä¸ªæ‚£è€…")
        print(f"   - æ€»å½•åˆ¶æ—¶é—´: {original_stats['total_duration_hours']:.1f} å°æ—¶")
        print(f"   - ç™«ç—«äº‹ä»¶: {original_stats['seizure_summary']['total_seizures']}")
        print(f"   - ä½¿ç”¨ {mp.cpu_count()} ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œå¤„ç†")
    else:
        print("âŒ åŸå§‹EDFæ–‡ä»¶: æœªæ‰¾åˆ°æˆ–æ— æ³•è®¿é—®")
        print("   - åˆ†æä»…åŸºäºä»£ç ç»“æ„")
    
    print("âœ… é¢„å¤„ç†æµæ°´çº¿: ä»ä»£ç åˆ†æ")
    print(f"   - ä¼°è®¡è¾“å‡º: {sum(estimated_segments.values()):,} ä¸ªåˆ†æ®µ")
    print(f"   - å†…å­˜ä¼°è®¡: ~{sum(estimated_segments.values())*19*10*200*4/1024/1024/1024:.1f} GB")
    
    print("âœ… æ–‡æ¡£: å·²ç”Ÿæˆ")
    print(f"   - MarkdownæŠ¥å‘Š: {analyzer.output_dir}/CHB_MIT_è¯¦ç»†åˆ†æ.md")
    print(f"   - å¯è§†åŒ–å›¾è¡¨: {analyzer.output_dir}/chb_mit_è¯¦ç»†åˆ†æ.png")
